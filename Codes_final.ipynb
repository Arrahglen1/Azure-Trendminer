{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dateutil.parser import parse \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.random import randn\n",
    "from pandas import Series, DataFrame\n",
    "from pylab import rcParams\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "###\n",
    "from pandas import read_csv\n",
    "from pandas import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\arrah\\Desktop\\Trendminer_Intern\\data_one_day.csv\", parse_dates =[\"timestamp\"])\n",
    "\n",
    "#df_large = pd.read_csv(r\"C:\\Users\\arrah\\Desktop\\Trendminer_Intern\\CoffeeML-07-20_08-08.csv\",index_col=0,squeeze=True,parse_dates =[\"timestamp\"])\n",
    "\n",
    "count_row = df.shape[0]  # gives number of row count\n",
    "count_column = df.shape[1] # gives number of column count\n",
    "\n",
    "print('Number of rows: {}'.format(count_row))\n",
    "print('Number of columns: {}'.format(count_column))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specifying the range of time needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data_0 = df1.drop(df1['2020-07-16 00:00:00':'2020-07-16 07:50:00'].index)\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "startDate=['2020-09-02 06:00:00']\n",
    "endDate=['2020-09-02 19:00:00']\n",
    "#data_0[data_0.timestamp<endDate & startDate]\n",
    "#df[df['timestamp']>= startDate & df ['timestamp']<= endDate]\n",
    "result = []\n",
    "for i in  range(0, len(startDate)) :\n",
    "    start = startDate[i]\n",
    "    end = endDate[i]\n",
    "    result.append((df['timestamp'] > start) & (df['timestamp'] <= end)) \n",
    "\n",
    "resultDf = []\n",
    "for item in result:\n",
    "    if len(item)>0:\n",
    "        resultDf.append(df.loc[item])\n",
    "\n",
    "df_select = pd.concat(resultDf)\n",
    "df_select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check for missing data\n",
    "df_select.isnull().sum() # gives number of null value by column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setting the timestamp variable as index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_index=df_select.set_index('timestamp')\n",
    "df_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_index.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Resampling the datasets in 100ml\n",
    "df_resample=df_index.resample('100ms',level=0).mean()\n",
    "df_resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Splitting the dataset into dependent variable and independent variables, so as to apply interpollation\n",
    "X_i, y_i = df_resample.drop(['Keypad.keyPressed'], axis=1), df_resample['Keypad.keyPressed']\n",
    "X_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#interpollation of the independent variables\n",
    "X_inter = X_i.interpolate(method='linear', axis=0).ffill().bfill()\n",
    "X_inter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# concantenate the interpollated dataset to y_i\n",
    "df_con = pd.concat([X_inter, y_i], axis=1)\n",
    "df_con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Boxplot\n",
    "import seaborn as sb\n",
    "sb.boxplot(x='Keypad.keyPressed', y='AnalogNoiseSensor.noiseTotal', data=df_con, palette='hls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation plot\n",
    "plt.figure(figsize=(7,7))\n",
    "p=sns.heatmap(X_inter.corr(), annot=True,cmap='RdYlGn',square=True, fmt='.2f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting our variables of interest\n",
    "df_extr = df_con[['AnalogNoiseSensor.noiseTotal','Keypad.keyPressed']]\n",
    "df_extr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# resetting the index column as a variable\n",
    "df_reset= df_extr.reset_index()\n",
    "df_reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# delete all rows with column 'Keypad.keyPressed having  values 8 to 9 \n",
    "df_reset1= df_reset[~df_reset['Keypad.keyPressed'].isin([8,9])] \n",
    "df_reset1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#g = df_reset1.to_csv(r\"C:\\Users\\arrah\\Desktop\\Trendminer_Intern\\data_one_day_transformed.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "fig = px.line(df_reset1, x='timestamp',y = 'AnalogNoiseSensor.noiseTotal',title= 'timestamp vs AnalogNoiseSensor.noiseTotal with slider')\n",
    "\n",
    "fig.update_xaxes(\n",
    "     rangeslider_visible=True,\n",
    "     rangeselector=dict(\n",
    "       \n",
    "     )\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reset2=df_reset1[35000:40000]\n",
    "df_reset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import plotly.express as px\n",
    "# fig = px.line(db, x='timestamp',y = 'AnalogNoiseSensor.noiseTotal',title= 'timestamp vs AnalogNoiseSensor.noiseTotal with slider')\n",
    "\n",
    "# fig.update_xaxes(\n",
    "#      rangeslider_visible=True,\n",
    "#      rangeselector=dict(\n",
    "#         buttons=list([\n",
    "#             dict(count=1, label=\"1 day\", step=\"day\", stepmode=\"backward\"),\n",
    "#             dict(count=2, label=\"4 day\", step=\"day\", stepmode=\"backward\"),\n",
    "#             dict(count=3, label=\"7 day\", step=\"day\", stepmode=\"backward\"),\n",
    "#             dict(step=\"all\")\n",
    "#         ])\n",
    "#      )\n",
    "# )\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_analog =df_reset[['AnalogNoiseSensor.noiseTotal']]\n",
    "# df_analog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sliding window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_len=5000\n",
    "global_k=60\n",
    "# Applying only for the variable analognoise.total\n",
    "\n",
    "def generate_sample(k, sample_list):\n",
    "    result = []\n",
    "    for i in range(len(sample_list)):\n",
    "        if i < len(sample_list) - k:\n",
    "            result.append(sample_list[i : i+k])           \n",
    "    temp = np.array(result)\n",
    "    final_result = np.reshape(temp, (1, len(sample_list)-k, k))\n",
    "    return final_result\n",
    "    \n",
    "\n",
    "\n",
    "data_matrix = df_reset2[\"AnalogNoiseSensor.noiseTotal\"].values\n",
    "\n",
    "sampled_matrix_of_k = generate_sample(60, data_matrix)\n",
    "\n",
    "new_analog= pd.DataFrame(sampled_matrix_of_k[0])\n",
    "new_analog\n",
    "#sampled_matrix_of_k[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying only for the variable keypad pressed\n",
    "import math\n",
    "\n",
    "def sliding_window_of_values(key_pad_frame, k):\n",
    "    keypad_matrix = key_pad_frame.values\n",
    "    \n",
    "    r = []\n",
    "    for i in range(len(keypad_matrix) - k):\n",
    "        print(keypad_matrix[i : i+k])\n",
    "        for y in range(len(keypad_matrix[i : i+k])):\n",
    "            if not math.isnan(keypad_matrix[i : i+k][y]):\n",
    "            \n",
    "                r.append(keypad_matrix[i : i+k][y])\n",
    "                break\n",
    "            elif y == len(keypad_matrix[i : i+k]) - 1:\n",
    "                r.append(keypad_matrix[i : i+k][y])\n",
    "        \n",
    "    return r\n",
    "                \n",
    "test_dataframe = df_reset2[\"Keypad.keyPressed\"]\n",
    "\n",
    "keypadd = sliding_window_of_values(test_dataframe, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypadd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(keypadd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_timestamp =df_reset[[\"timestamp\"]][:global_len-global_k]\n",
    "df_timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concantenating the frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_analog['keypad'] = keypadd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_analog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_frame = pd.concat([df_timestamp, new_analog], axis=1)\n",
    "merged_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts the number of occurences greater than target_value in a list of integer\n",
    "def count_occurence_in_row(row, target_value):\n",
    "    count = 0\n",
    "    for i in range(len(row)):\n",
    "        if row[i] > target_value:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns true if the count in a row is greater or equal than the cutt_off_pt\n",
    "def is_valid_row(row, target_value, cut_off_pt):\n",
    "    if count_occurence_in_row(row, target_value) >= cut_off_pt:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a  new dataframe satisfying the conditions of the target avlue and cutt_off_pt \n",
    "\n",
    "def create_data_frame_with_valid_row(sample_data_frame, cutt_off_pt, target_value):\n",
    "    analog_df_values = sample_data_frame.drop(columns=['timestamp', 'keypad']).values\n",
    "    invalid_indexes = []\n",
    "    for index in range(len(analog_df_values)):\n",
    "        if not is_valid_row(analog_df_values[index], target_value, cutt_off_pt):\n",
    "            invalid_indexes.append(index)\n",
    "    return sample_data_frame.drop(sample_data_frame.index[invalid_indexes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_frame = create_data_frame_with_valid_row(merged_frame, cutt_off_pt=45, target_value=15300)\n",
    "\n",
    "result_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#g = result_frame.to_csv(r\"C:\\Users\\arrah\\Desktop\\Trendminer_Intern\\exploratory.csv\", index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the index as a list to do the filtering\n",
    "result_frame.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function uses the list of index and takes the largest from the first column(zero) \n",
    "#of consecutive sequence base on the difference we chose\n",
    "\n",
    "def drop_consecutive_row(input_df, delta ):\n",
    "    index_list = input_df.index.tolist()\n",
    "    index_to_keep = []\n",
    "    start = 0\n",
    "    for i in range(len(index_list)-1):\n",
    "        if index_list[i+1] - index_list[i] > delta:\n",
    "            index_to_keep.append(input_df[0][start:i].idxmax(axis=0))\n",
    "            start = i+1\n",
    "            \n",
    "    return input_df.loc[index_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "selecte_frame = drop_consecutive_row(result_frame, 100)\n",
    "selecte_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#g = df_one_day.to_csv(r\"C:\\Users\\arrah\\Desktop\\Trendminer_Intern\\final_dataframe\\Data_one_day_70_15200_45_60_final.csv\", index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_part1 = pd.read_csv(r\"C:\\Users\\arrah\\Desktop\\Trendminer_Intern\\final_dataframe\\new\\After_df_part1_70_15200_45_70.csv\")\n",
    "# df_part1 = df_part1.drop(['Unnamed: 0'], axis=1)\n",
    "# df_part1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_part2 = pd.read_csv(r\"C:\\Users\\arrah\\Desktop\\Trendminer_Intern\\final_dataframe\\new\\After_df_part2_70_15200_45_70.csv\")\n",
    "# df_part2 = df_part2.drop(['Unnamed: 0'], axis=1)\n",
    "# df_part2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_part3 = pd.read_csv(r\"C:\\Users\\arrah\\Desktop\\Trendminer_Intern\\final_dataframe\\new\\After_df_part3_70_15200_45_70.csv\")\n",
    "# df_part3 = df_part3.drop(['Unnamed: 0'], axis=1)\n",
    "# df_part3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_part11 = pd.read_csv(r\"C:\\Users\\arrah\\Desktop\\Trendminer_Intern\\final_dataframe\\new\\After_df_part11_70_15200_45_70.csv\")\n",
    "# df_part11 = df_part11.drop(['Unnamed: 0'], axis=1)\n",
    "# df_part11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df_one_day = pd.read_csv(r\"C:\\Users\\arrah\\Desktop\\Trendminer_Intern\\final_dataframe\\Data_one_day_70_15200_45_60_final.csv\")\n",
    "# df_one_day.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# dropping columns not needed\n",
    "df_one_day1 = df_one_day.drop(['Unnamed: 0','Unnamed: 0.1'], axis=1)\n",
    "df_one_day1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_append = pd.concat([df_part1,df_part2,df_part3,df_part11, df_one_day1])\n",
    "# df_append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_append = pd.read_csv(r\"C:\\Users\\arrah\\Desktop\\Trendminer_Intern\\final_dataframe\\new\\df_join1.csv\")\n",
    "df_append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# splitting the dataset to apply clustering on the predictors \n",
    "X_indep , Y_dep= df_append1.drop(['timestamp', 'keypad'], axis=1),df_append1['keypad']\n",
    "X_indep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setting up for cluster Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.preprocessing import scale\n",
    "import sklearn.metrics as sm\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(7,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "#scaler = StandardScaler()\n",
    "\n",
    "# Optionnal\n",
    "X_scale= pd.DataFrame(scaler.fit_transform(X_indep), columns=X_indep.columns)\n",
    "X_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and running your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = 5\n",
    "kmeans = KMeans(n_clusters = clusters, random_state=123) \n",
    "y_assign=kmeans.fit(X_scale) \n",
    "y_assign\n",
    "#print(y_assign.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=5, random_state=123)\n",
    "kmeans.fit(X_scale)\n",
    "y_kmeans = kmeans.predict(X_scale)\n",
    "y_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## count of each class\n",
    "frame = pd.DataFrame(X_scale)\n",
    "frame['cluster'] = y_kmeans\n",
    "frame['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_append['cluster'] = y_kmeans\n",
    "df_append.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierachical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "%matplotlib inline \n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.cluster import AgglomerativeClustering \n",
    "from sklearn.preprocessing import StandardScaler, normalize \n",
    "from sklearn.metrics import silhouette_score \n",
    "import scipy.cluster.hierarchy as shc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sts = X_scale.iloc[:,:].values\n",
    "sts[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLUSTERING Create Linkage Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = linkage(sts, 'ward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Dendrogram of Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (25, 10))\n",
    "plt.title('Hierachical Cluster Dendrogram ')\n",
    "plt.axhline(y=5, color='r', linestyle='--')\n",
    "#plt.ylabel('distance')\n",
    "dendrogram(\n",
    "    Z,\n",
    "    labels =X_scale.index,\n",
    "    leaf_rotation = 0.,\n",
    "    leaf_font_size = 18.,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "cluster_h = AgglomerativeClustering(n_clusters=5, affinity='euclidean', linkage='ward')  \n",
    "hl = cluster_h.fit_predict(sts)\n",
    "hl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## count of each class for hierachical clustering\n",
    "frame = pd.DataFrame(X_scale)\n",
    "frame['cluster'] = hl\n",
    "frame['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_append['cluster'] = hl\n",
    "df_append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(sts[:,0], sts[:,0], c=cluster_h.labels_, cmap='rainbow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA   \n",
    "pca = PCA(0.8, svd_solver='full') \n",
    "pca.fit(X_scale) \n",
    "  \n",
    "pca_data = pd.DataFrame(pca.transform(X_scale)) \n",
    "  \n",
    "pca_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pca = pd.concat([pca_data,y], axis=1) \n",
    "# df_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.singular_values_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random oversampling to balance the class distribution\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "X_o, y_o= make_classification(n_samples=61, n_features=19, n_informative=3,\n",
    "                            n_redundant=0, n_repeated=0, n_classes=5,\n",
    "                            n_clusters_per_class=1,\n",
    "                            weights=[0.48, 0.28, 0.13, 0.07, 0.04],\n",
    "                            class_sep=0.8, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_pca_o = pd.DataFrame(X_o)\n",
    "df_pca_o.columns = ['0','1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18']\n",
    "df_pca_o['keypad']= y\n",
    "df_pca_o.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_pca_o['keypad'].astype(str).hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#apply the Random over-sampling approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_oversample, y_train_oversample = ros.fit_resample(X_o, y_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca_o = pd.DataFrame(y_train_oversample)\n",
    "df_pca_o[0].astype(str).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bal= pd.Series(data=y_resampled.flatten())\n",
    "df_bal.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling Imbalance using SMOTE\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from sklearn.datasets import make_classification\n",
    "from matplotlib import pyplot\n",
    "from collections import Counter\n",
    "from numpy import where\n",
    "#pip install imblearn\n",
    "#pip install imbalanced-learn\n",
    "smote = SMOTE(sampling_strategy = 'minority')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataset\n",
    "X_sm, y_sm = make_classification(n_samples=61, n_features=19, n_redundant=0, n_classes=5, n_informative=3,\n",
    "n_clusters_per_class=1, weights=[0.48, 0.28, 0.13, 0.07, 0.04], flip_y=0, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_pca_sm = pd.DataFrame(X_sm)\n",
    "df_pca_sm.columns = ['0','1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18']\n",
    "df_pca_sm['keypad']= y\n",
    "df_pca_sm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca_sm['keypad'].astype(str).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize class distribution\n",
    "counter = Counter(y_sm)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the dataset\n",
    "sm = SMOTE(sampling_strategy='auto', k_neighbors=1, random_state=123)\n",
    "X_smote, y_smote = sm.fit_resample(X_sm, y_sm)\n",
    "# summarize the new class distribution\n",
    "counter = Counter(y_smote)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca_sm = pd.DataFrame(y_smote)\n",
    "df_pca_sm[0].astype(str).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_smote, y_train_smote = smote.fit_resample(X_smote, y_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # example of evaluating a model with random oversampling and undersampling\n",
    "# from numpy import mean\n",
    "# from sklearn.datasets import make_classification\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from imblearn.pipeline import Pipeline\n",
    "# from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define pipeline\n",
    "# over = RandomOverSampler(sampling_strategy=0.1)\n",
    "# steps = [('o', over), ('m', DecisionTreeClassifier())]\n",
    "# pipeline = Pipeline(steps=steps)\n",
    "# # evaluate pipeline\n",
    "# cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=123)\n",
    "# scores = cross_val_score(pipeline, X, y, scoring='f1_micro', cv=cv, n_jobs=-1)\n",
    "# score = mean(scores)\n",
    "# print('F1 Score: %.3f' % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " # drop NaN values from df_append on the keypad to apply classification modelling on the label data\n",
    "df_append1 = df_append.dropna()\n",
    "df_append1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k=pd.DataFrame(data=y_smote.flatten())\n",
    "# k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "X, y = pca_data , df_append1['keypad']\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "#scaler = StandardScaler()\n",
    "\n",
    "# Optionnal\n",
    "X = pd.DataFrame(scaler.fit_transform(X), index=X.index, columns=X.columns)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# X, y = df_append1.drop(['timestamp','keypad'], axis=1) , df_append1['keypad']\n",
    "\n",
    "# scaler = MinMaxScaler()\n",
    "# #scaler = StandardScaler()\n",
    "\n",
    "# # Optionnal\n",
    "# X = pd.DataFrame(scaler.fit_transform(X), index=X.index, columns=X.columns)\n",
    "# X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPLIT TRAIN\\TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_oversample, y_train_oversample , test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(C=1,solver='lbfgs',max_iter=100)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "preds = clf.predict(X_test)\n",
    "#print('Test set predictions: {}'.format(preds))\n",
    "#print('\\n')\n",
    "\n",
    "#print('Train set accuracy: {:.2f}'.format(accuracy_score(y_train, clf.predict(X_train))))\n",
    "#print('Test set accuracy: {:.2f}'.format(accuracy_score(y_test, clf.predict(X_test))))\n",
    "print('Train set accuracy: {:.2f}'.format(clf.score(X_train, y_train)))\n",
    "print('Test set accuracy: {:.2f}'.format(clf.score(X_test, y_test)))\n",
    "print('\\n')\n",
    "\n",
    "confusion = confusion_matrix(y_test, preds)\n",
    "\n",
    "print('Confusion matrix:\\n', confusion)\n",
    "\n",
    "# plt.bar(range(X_train.shape[1]), clf.coef_[0], align = 'center', alpha = 0.5)\n",
    "# plt.xticks(range(X_train.shape[1]), X_train.columns, rotation=90)\n",
    "# xlims = plt.xlim()\n",
    "# plt.hlines(0, xlims[0], xlims[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree\n",
    "\n",
    "### Predictive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=16, min_samples_leaf=1)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "preds = clf.predict(X_test)\n",
    "#print('Test set predictions: {}'.format(preds))\n",
    "#print('\\n')\n",
    "\n",
    "#print('Train set accuracy: {:.2f}'.format(accuracy_score(y_train, clf.predict(X_train))))\n",
    "#print('Test set accuracy: {:.2f}'.format(accuracy_score(y_test, clf.predict(X_test))))\n",
    "print('Train set accuracy: {:.2f}'.format(clf.score(X_train, y_train)))\n",
    "print('Test set accuracy: {:.2f}'.format(clf.score(X_test, y_test)))\n",
    "print('\\n')\n",
    "\n",
    "confusion = confusion_matrix(y_test, preds)\n",
    "\n",
    "print('Confusion matrix:\\n', confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train models\n",
    "\n",
    "## k-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "n = 2\n",
    "\n",
    "clf = KNeighborsClassifier(n)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "preds = clf.predict(X_test)\n",
    "print('Test set predictions: {}'.format(preds))\n",
    "print('\\n')\n",
    "\n",
    "#print('Train set accuracy: {:.2f}'.format(accuracy_score(y_train, clf.predict(X_train))))\n",
    "#print('Test set accuracy: {:.2f}'.format(accuracy_score(y_test, clf.predict(X_test))))\n",
    "print('Train set accuracy: {:.2f}'.format(clf.score(X_train, y_train)))\n",
    "print('Test set accuracy: {:.2f}'.format(clf.score(X_test, y_test)))\n",
    "print('\\n')\n",
    "\n",
    "confusion = confusion_matrix(y_test, preds)\n",
    "\n",
    "print('Confusion matrix:\\n', confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the best value for n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_accuracy = []\n",
    "test_accuracy = []\n",
    "\n",
    "n_range = range(1, 11)\n",
    "\n",
    "for n in n_range:\n",
    "    clf = KNeighborsClassifier(n)\n",
    "    clf.fit(X_train, y_train)\n",
    "    training_accuracy.append(clf.score(X_train, y_train))\n",
    "    test_accuracy.append(clf.score(X_test, y_test))\n",
    "    \n",
    "plt.plot(n_range, training_accuracy, label='training accuracy')\n",
    "plt.plot(n_range, test_accuracy, label='test accuracy')\n",
    "plt.xlabel('n neighbors')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = SVC(kernel=\"linear\", C=.1)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "preds = clf.predict(X_test)\n",
    "#print('Test set predictions: {}'.format(preds))\n",
    "#print('\\n')\n",
    "\n",
    "#print('Train set accuracy: {:.2f}'.format(accuracy_score(y_train, clf.predict(X_train))))\n",
    "#print('Test set accuracy: {:.2f}'.format(accuracy_score(y_test, clf.predict(X_test))))\n",
    "print('Train set accuracy: {:.2f}'.format(clf.score(X_train, y_train)))\n",
    "print('Test set accuracy: {:.2f}'.format(clf.score(X_test, y_test)))\n",
    "print('\\n')\n",
    "\n",
    "confusion = confusion_matrix(y_test, preds)\n",
    "\n",
    "print('Confusion matrix:\\n', confusion)\n",
    "\n",
    "# plt.bar(range(X_train.shape[1]), clf.coef_[0], align = 'center', alpha = 0.5)\n",
    "# plt.xticks(range(X_train.shape[1]), X_train.columns, rotation=90)\n",
    "# xlims = plt.xlim()\n",
    "# plt.hlines(0, xlims[0], xlims[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the best value of C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_accuracy = []\n",
    "test_accuracy = []\n",
    "\n",
    "C_range = np.logspace(-3, 3, num=7)\n",
    "\n",
    "for C in C_range:\n",
    "    clf = SVC(kernel=\"linear\", C=C)\n",
    "    clf.fit(X_train, y_train)\n",
    "    training_accuracy.append(clf.score(X_train, y_train))\n",
    "    test_accuracy.append(clf.score(X_test, y_test))\n",
    "    \n",
    "plt.plot(C_range, training_accuracy, label='training accuracy')\n",
    "plt.plot(C_range, test_accuracy, label='test accuracy')\n",
    "plt.xlabel('C')\n",
    "plt.xscale(\"log\")\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(y_test,preds))\n",
    "print(classification_report(y_test,preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into train, validation, and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(y), len(y_train), len(y_val), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LogReg = LogisticRegression(multi_class='multinomial',max_iter=300)\n",
    "LogReg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = cross_val_predict(LogReg, X_train, y_train, cv=3)\n",
    "confusion_matrix(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_train, y_train_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # the function uses the list and takes the first of consecutive sequence base on the difference we chose\n",
    "# def drop_consecutive_row(input_df, delta ):\n",
    "#     index_list = input_df.index.tolist()\n",
    "#     index_to_keep = [index_list[0]]\n",
    "#     for i in range(len(index_list)-1):\n",
    "#         if index_list[i+1] - index_list[i] > delta:\n",
    "#             index_to_keep.append(index_list[i+1])\n",
    "#     return input_df.loc[index_to_keep]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
